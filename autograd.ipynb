{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import string\n",
    "import sympy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('a b c d e f g h i j k l m', 'a,b,c,d,e,f,g,h,i,j,k,l,m')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(string.ascii_letters[:13]),\",\".join(string.ascii_letters[:13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c,d,e,f,g,h,i,j,k,l,m = sp.symbols('a b c d e f g h i j k l m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_grad_fn(a:torch.Tensor,name:str):\n",
    "    print(f\"grad fn of {name} : {g.name() if(g:=a.grad_fn) else g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad fn of a : None\n",
      "grad fn of b : None\n"
     ]
    }
   ],
   "source": [
    "a_tensor = torch.tensor(2.0)\n",
    "b_tensor = torch.tensor(2.0)\n",
    "\n",
    "\n",
    "print_grad_fn(a_tensor,\"a\")\n",
    "print_grad_fn(b_tensor,\"b\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad fn of c : None\n"
     ]
    }
   ],
   "source": [
    "c_tensor = a_tensor * b_tensor\n",
    "\n",
    "c_tensor.requires_grad = True\n",
    "print_grad_fn(c_tensor,\"c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad fn of d : None\n",
      "grad fn of e : MulBackward0\n"
     ]
    }
   ],
   "source": [
    "d_tensor = torch.tensor(2.0)\n",
    "\n",
    "e_tensor = c_tensor *d_tensor\n",
    "e = c * d\n",
    "\n",
    "print_grad_fn(d_tensor,\"d\")\n",
    "print_grad_fn(e_tensor,\"e\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle c d$"
      ],
      "text/plain": [
       "c*d"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why we are storing the value d only not c?\n",
    "1. Only `C` has the require_grad = True and its also leaf.\n",
    "2. By default grad will accumulated on the leaf .\n",
    "3. in order to accumulate grad on the non leaf node of tree we need to use `retain_grad`\n",
    "4. When we differentiate the `E wrt C` is `D` refer sympy for that\n",
    "5. That the reason we are storing the D in saved_other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(e_tensor.grad_fn._saved_other) == id(d_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle d$"
      ],
      "text/plain": [
       "d"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.diff(e,c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when we do backward on E, the grad will is accumulated in C.grad which is not nothing but the value of D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((<AccumulateGrad at 0x7ff25823d120>, 0), (None, 0))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_tensor.grad_fn.next_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before e.backward() :  None\n",
      "After  e.backward() :  tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "print(\"Before e.backward() : \",c_tensor.grad)\n",
    "e_tensor.backward()\n",
    "print(\"After  e.backward() : \",c_tensor.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why only leaf get the grad default?\n",
    "\n",
    "All parameter in model is initialized , it **NOT** create by the operations. That why the grad is accumulated only on the leaf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad fn of f : None\n",
      "grad fn of g : MulBackward0\n"
     ]
    }
   ],
   "source": [
    "f_tensor = torch.tensor(2.0)\n",
    "g_tensor = e_tensor * f_tensor\n",
    "\n",
    "print_grad_fn(f_tensor,\"f\")\n",
    "print_grad_fn(g_tensor,\"g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((<MulBackward0 at 0x7ff1652bb340>, 0), (None, 0))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_tensor.grad_fn.next_functions\n",
    "\n",
    "# for input e --> MulBackward\n",
    "# for input f --> No function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(g_tensor.grad_fn.next_functions[0][0]) == id(e_tensor.grad_fn)\n",
    "\n",
    "# Why there is no accumulate grad for E even though it require grad ?\n",
    "# this is because by default only leave node get the grad accumulated "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grad Accumulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_tensor = torch.tensor(2.0, requires_grad=True)\n",
    "b_tensor = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "c_tensor = a_tensor + b_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4., grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_tensor.grad_fn._saved_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None None\n",
      "tensor(1.) tensor(1.)\n",
      "1 1\n"
     ]
    }
   ],
   "source": [
    "print(a_tensor.grad,b_tensor.grad)\n",
    "c_tensor.backward(retain_graph=True)\n",
    "print(a_tensor.grad,b_tensor.grad)\n",
    "\n",
    "a,b,c = sp.symbols(\"a b c\")\n",
    "c = a + b\n",
    "print(sp.diff(c,a),sp.diff(c,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.) tensor(1.)\n",
      "tensor(6.) tensor(6.)\n"
     ]
    }
   ],
   "source": [
    "d_tensor = 5 * c_tensor\n",
    "\n",
    "print(a_tensor.grad,b_tensor.grad)\n",
    "d_tensor.backward(retain_graph=True)\n",
    "print(a_tensor.grad,b_tensor.grad)\n",
    "# grad accumulated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, tensor(5))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_tensor.grad_fn._saved_alpha,d_tensor.grad_fn._saved_other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 5)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a,b,c,d = sp.symbols(\"a b c d\")\n",
    "d = 5 * (a + b)\n",
    "sp.diff(d,a) ,sp.diff(d,b) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom AutoGrad Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Exp(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx,i):\n",
    "        print(\"Calling forward\")\n",
    "        for attr in dir(ctx):\n",
    "            if not attr.startswith(\"__\") and not callable((a:=getattr(ctx,attr,None))):\n",
    "                print(f\"{attr:<40s}:{a}\")\n",
    "        result = i.exp()\n",
    "        ctx.save_for_backward(result)\n",
    "        return result\n",
    "    @staticmethod\n",
    "    def backward(ctx,grad_output):\n",
    "        print(\"Calling backward\")\n",
    "        for attr in dir(ctx):\n",
    "            if not attr.startswith(\"__\") and not callable((a:=getattr(ctx,attr,None))):\n",
    "                print(f\"{attr:<40s}:{a}\")\n",
    "        result, = ctx.saved_tensors\n",
    "        return grad_output * result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling forward\n",
      "_materialize_non_diff_grads             :True\n",
      "_raw_saved_tensors                      :()\n",
      "dirty_tensors                           :None\n",
      "materialize_grads                       :None\n",
      "metadata                                :{}\n",
      "needs_input_grad                        :(True,)\n",
      "next_functions                          :((<AccumulateGrad object at 0x7ff1652ba9e0>, 0),)\n",
      "non_differentiable                      :None\n",
      "requires_grad                           :True\n",
      "saved_for_forward                       :None\n",
      "saved_tensors                           :()\n",
      "saved_variables                         :()\n",
      "to_save                                 :None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32725/2961181535.py:6: DeprecationWarning: 'saved_variables' is deprecated; use 'saved_tensors'\n",
      "  if not attr.startswith(\"__\") and not callable((a:=getattr(ctx,attr,None))):\n"
     ]
    }
   ],
   "source": [
    "in_tensor = torch.tensor(2.0,requires_grad=True)\n",
    "out_tensor = Exp.apply(in_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before out.backward() :  None\n",
      "Calling backward\n",
      "_materialize_non_diff_grads             :True\n",
      "_raw_saved_tensors                      :(<torch._C._autograd.SavedTensor object at 0x7ff25a3c72f0>,)\n",
      "dirty_tensors                           :None\n",
      "materialize_grads                       :None\n",
      "metadata                                :{}\n",
      "needs_input_grad                        :(True,)\n",
      "next_functions                          :((<AccumulateGrad object at 0x7ff25823cc10>, 0),)\n",
      "non_differentiable                      :None\n",
      "requires_grad                           :True\n",
      "saved_for_forward                       :None\n",
      "saved_tensors                           :(tensor(7.3891, grad_fn=<ExpBackward>),)\n",
      "saved_variables                         :(tensor(7.3891, grad_fn=<ExpBackward>),)\n",
      "to_save                                 :None\n",
      "After  out.backward() :  tensor(7.3891)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32725/2961181535.py:15: DeprecationWarning: 'saved_variables' is deprecated; use 'saved_tensors'\n",
      "  if not attr.startswith(\"__\") and not callable((a:=getattr(ctx,attr,None))):\n"
     ]
    }
   ],
   "source": [
    "print(\"Before out.backward() : \",in_tensor.grad)\n",
    "out_tensor.backward()\n",
    "print(\"After  out.backward() : \",in_tensor.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_tensor = torch.tensor(2.0, requires_grad=True)\n",
    "b_tensor = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "class Addition(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx,a,b):\n",
    "        return a+b\n",
    "    @staticmethod\n",
    "    def backward(ctx,grad):\n",
    "        # print(ctx.needs_input_grad)\n",
    "        # print(grad)\n",
    "        return grad,grad\n",
    "        \n",
    "\n",
    "c_tensor = Addition.apply(a_tensor,b_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4., grad_fn=<AdditionBackward>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None None\n",
      "tensor(1.) tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "print(a_tensor.grad,b_tensor.grad)\n",
    "c_tensor.backward(retain_graph=True)\n",
    "print(a_tensor.grad,b_tensor.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.) tensor(1.)\n",
      "tensor(6.) tensor(6.)\n"
     ]
    }
   ],
   "source": [
    "d_tensor = 5 * c_tensor\n",
    "\n",
    "print(a_tensor.grad,b_tensor.grad)\n",
    "d_tensor.backward(retain_graph=True)\n",
    "print(a_tensor.grad,b_tensor.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def helper_fn(**kwargs):\n",
    "    id_dict = {}\n",
    "    for name,tensor in kwargs.items():\n",
    "        id_dict[id(tensor)] = name\n",
    "    id_dict[id(None)] = \"None\"\n",
    "    for name,tensor in kwargs.items():\n",
    "        print(f\"{name}: {id(tensor)}\")\n",
    "        print(f\"   {'Shape :':>30s} {tensor.shape}\")\n",
    "        print(f\"   {'Leaf :':>30s} {tensor.is_leaf}\")\n",
    "        print(f\"   {'Req Grad :':>30s} {tensor.requires_grad}\")\n",
    "        print(f\"   {'Grad fn :':>30s} {a.name() if (a:=tensor.grad_fn) else a}\")\n",
    "        print(f\"   {'Grad :':>30s} {id_dict.get(id(tensor.grad),id(tensor.grad))}\")\n",
    "        if a:\n",
    "            for attr in dir(a):\n",
    "                if attr.startswith(\"_saved\"):\n",
    "                    print(f\"   {attr[1:]+' :':>30s} {id_dict.get(id(getattr(a,attr,None)),id(getattr(a,attr,None)))}\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "inputs = torch.rand(1,1,device=\"cuda\")\n",
    "linear = torch.nn.Linear(1,2,bias=True,device=\"cuda\")\n",
    "out = linear(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out: 140388992467168\n",
      "                          Shape : torch.Size([1, 2])\n",
      "                           Leaf : False\n",
      "                       Req Grad : True\n",
      "                        Grad fn : AddmmBackward0\n",
      "                           Grad : None\n",
      "                    saved_alpha : 140393301885168\n",
      "                     saved_beta : 140393301885168\n",
      "                     saved_mat1 : inputs\n",
      "           saved_mat1_sym_sizes : 140388890857472\n",
      "         saved_mat1_sym_strides : 140393301901424\n",
      "                     saved_mat2 : None\n",
      "           saved_mat2_sym_sizes : 140388890857472\n",
      "         saved_mat2_sym_strides : 140388890857472\n",
      "inputs: 140388992552608\n",
      "                          Shape : torch.Size([1, 1])\n",
      "                           Leaf : True\n",
      "                       Req Grad : False\n",
      "                        Grad fn : None\n",
      "                           Grad : None\n",
      "weight: 140388890914192\n",
      "                          Shape : torch.Size([2, 1])\n",
      "                           Leaf : True\n",
      "                       Req Grad : True\n",
      "                        Grad fn : None\n",
      "                           Grad : None\n",
      "bias: 140388890914592\n",
      "                          Shape : torch.Size([2])\n",
      "                           Leaf : True\n",
      "                       Req Grad : True\n",
      "                        Grad fn : None\n",
      "                           Grad : None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_627/1462208218.py:12: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n",
      "  print(f\"   {'Grad :':>30s} {id_dict.get(id(tensor.grad),id(tensor.grad))}\")\n"
     ]
    }
   ],
   "source": [
    "helper_fn(out=out,inputs=inputs,weight=linear.weight,bias=linear.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out: 140388992467168\n",
      "                          Shape : torch.Size([1, 2])\n",
      "                           Leaf : False\n",
      "                       Req Grad : True\n",
      "                        Grad fn : AddmmBackward0\n",
      "                           Grad : None\n",
      "                    saved_alpha : 140393301885168\n",
      "                     saved_beta : 140393301885168\n",
      "                     saved_mat1 : inputs\n",
      "           saved_mat1_sym_sizes : 140388888685376\n",
      "         saved_mat1_sym_strides : 140393301901424\n",
      "                     saved_mat2 : None\n",
      "           saved_mat2_sym_sizes : 140388888685376\n",
      "         saved_mat2_sym_strides : 140388888685376\n",
      "inputs: 140388992552608\n",
      "                          Shape : torch.Size([1, 1])\n",
      "                           Leaf : True\n",
      "                       Req Grad : False\n",
      "                        Grad fn : None\n",
      "                           Grad : None\n",
      "weight: 140388890914192\n",
      "                          Shape : torch.Size([2, 1])\n",
      "                           Leaf : True\n",
      "                       Req Grad : True\n",
      "                        Grad fn : None\n",
      "                           Grad : 140388890914672\n",
      "bias: 140388890914592\n",
      "                          Shape : torch.Size([2])\n",
      "                           Leaf : True\n",
      "                       Req Grad : True\n",
      "                        Grad fn : None\n",
      "                           Grad : 140388890914272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_627/1462208218.py:12: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n",
      "  print(f\"   {'Grad :':>30s} {id_dict.get(id(tensor.grad),id(tensor.grad))}\")\n"
     ]
    }
   ],
   "source": [
    "out.sum().backward(retain_graph=True)\n",
    "helper_fn(out=out,inputs=inputs,weight=linear.weight,bias=linear.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
