{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import string\n",
    "import sympy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('a b c d e f g h i j k l m', 'a,b,c,d,e,f,g,h,i,j,k,l,m')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(string.ascii_letters[:13]),\",\".join(string.ascii_letters[:13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c,d,e,f,g,h,i,j,k,l,m = sp.symbols('a b c d e f g h i j k l m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_grad_fn(a:torch.Tensor,name:str):\n",
    "    print(f\"grad fn of {name} : {g.name() if(g:=a.grad_fn) else g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad fn of a : None\n",
      "grad fn of b : None\n"
     ]
    }
   ],
   "source": [
    "a_tensor = torch.tensor(2.0)\n",
    "b_tensor = torch.tensor(2.0)\n",
    "\n",
    "\n",
    "print_grad_fn(a_tensor,\"a\")\n",
    "print_grad_fn(b_tensor,\"b\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad fn of c : None\n"
     ]
    }
   ],
   "source": [
    "c_tensor = a_tensor * b_tensor\n",
    "\n",
    "c_tensor.requires_grad = True\n",
    "print_grad_fn(c_tensor,\"c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad fn of d : None\n",
      "grad fn of e : MulBackward0\n"
     ]
    }
   ],
   "source": [
    "d_tensor = torch.tensor(2.0)\n",
    "\n",
    "e_tensor = c_tensor *d_tensor\n",
    "e = c * d\n",
    "\n",
    "print_grad_fn(d_tensor,\"d\")\n",
    "print_grad_fn(e_tensor,\"e\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle c d$"
      ],
      "text/plain": [
       "c*d"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why we are storing the value d only not c?\n",
    "1. Only `C` has the require_grad = True and its also leaf.\n",
    "2. By default grad will accumulated on the leaf .\n",
    "3. in order to accumulate grad on the non leaf node of tree we need to use `retain_grad`\n",
    "4. When we differentiate the `E wrt C` is `D` refer sympy for that\n",
    "5. That the reason we are storing the D in saved_other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(e_tensor.grad_fn._saved_other) == id(d_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle d$"
      ],
      "text/plain": [
       "d"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.diff(e,c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when we do backward on E, the grad will is accumulated in C.grad which is not nothing but the value of D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((<AccumulateGrad at 0x7ff25823d120>, 0), (None, 0))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_tensor.grad_fn.next_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before e.backward() :  None\n",
      "After  e.backward() :  tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "print(\"Before e.backward() : \",c_tensor.grad)\n",
    "e_tensor.backward()\n",
    "print(\"After  e.backward() : \",c_tensor.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why only leaf get the grad default?\n",
    "\n",
    "All parameter in model is initialized , it **NOT** create by the operations. That why the grad is accumulated only on the leaf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad fn of f : None\n",
      "grad fn of g : MulBackward0\n"
     ]
    }
   ],
   "source": [
    "f_tensor = torch.tensor(2.0)\n",
    "g_tensor = e_tensor * f_tensor\n",
    "\n",
    "print_grad_fn(f_tensor,\"f\")\n",
    "print_grad_fn(g_tensor,\"g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((<MulBackward0 at 0x7ff1652bb340>, 0), (None, 0))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_tensor.grad_fn.next_functions\n",
    "\n",
    "# for input e --> MulBackward\n",
    "# for input f --> No function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(g_tensor.grad_fn.next_functions[0][0]) == id(e_tensor.grad_fn)\n",
    "\n",
    "# Why there is no accumulate grad for E even though it require grad ?\n",
    "# this is because by default only leave node get the grad accumulated "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grad Accumulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_tensor = torch.tensor(2.0, requires_grad=True)\n",
    "b_tensor = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "c_tensor = a_tensor + b_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4., grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_tensor.grad_fn._saved_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None None\n",
      "tensor(1.) tensor(1.)\n",
      "1 1\n"
     ]
    }
   ],
   "source": [
    "print(a_tensor.grad,b_tensor.grad)\n",
    "c_tensor.backward(retain_graph=True)\n",
    "print(a_tensor.grad,b_tensor.grad)\n",
    "\n",
    "a,b,c = sp.symbols(\"a b c\")\n",
    "c = a + b\n",
    "print(sp.diff(c,a),sp.diff(c,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.) tensor(1.)\n",
      "tensor(6.) tensor(6.)\n"
     ]
    }
   ],
   "source": [
    "d_tensor = 5 * c_tensor\n",
    "\n",
    "print(a_tensor.grad,b_tensor.grad)\n",
    "d_tensor.backward(retain_graph=True)\n",
    "print(a_tensor.grad,b_tensor.grad)\n",
    "# grad accumulated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, tensor(5))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_tensor.grad_fn._saved_alpha,d_tensor.grad_fn._saved_other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 5)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a,b,c,d = sp.symbols(\"a b c d\")\n",
    "d = 5 * (a + b)\n",
    "sp.diff(d,a) ,sp.diff(d,b) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom AutoGrad Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Exp(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx,i):\n",
    "        print(\"Calling forward\")\n",
    "        for attr in dir(ctx):\n",
    "            if not attr.startswith(\"__\") and not callable((a:=getattr(ctx,attr,None))):\n",
    "                print(f\"{attr:<40s}:{a}\")\n",
    "        result = i.exp()\n",
    "        ctx.save_for_backward(result)\n",
    "        return result\n",
    "    @staticmethod\n",
    "    def backward(ctx,grad_output):\n",
    "        print(\"Calling backward\")\n",
    "        for attr in dir(ctx):\n",
    "            if not attr.startswith(\"__\") and not callable((a:=getattr(ctx,attr,None))):\n",
    "                print(f\"{attr:<40s}:{a}\")\n",
    "        result, = ctx.saved_tensors\n",
    "        return grad_output * result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling forward\n",
      "_materialize_non_diff_grads             :True\n",
      "_raw_saved_tensors                      :()\n",
      "dirty_tensors                           :None\n",
      "materialize_grads                       :None\n",
      "metadata                                :{}\n",
      "needs_input_grad                        :(True,)\n",
      "next_functions                          :((<AccumulateGrad object at 0x7ff1652ba9e0>, 0),)\n",
      "non_differentiable                      :None\n",
      "requires_grad                           :True\n",
      "saved_for_forward                       :None\n",
      "saved_tensors                           :()\n",
      "saved_variables                         :()\n",
      "to_save                                 :None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32725/2961181535.py:6: DeprecationWarning: 'saved_variables' is deprecated; use 'saved_tensors'\n",
      "  if not attr.startswith(\"__\") and not callable((a:=getattr(ctx,attr,None))):\n"
     ]
    }
   ],
   "source": [
    "in_tensor = torch.tensor(2.0,requires_grad=True)\n",
    "out_tensor = Exp.apply(in_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before out.backward() :  None\n",
      "Calling backward\n",
      "_materialize_non_diff_grads             :True\n",
      "_raw_saved_tensors                      :(<torch._C._autograd.SavedTensor object at 0x7ff25a3c72f0>,)\n",
      "dirty_tensors                           :None\n",
      "materialize_grads                       :None\n",
      "metadata                                :{}\n",
      "needs_input_grad                        :(True,)\n",
      "next_functions                          :((<AccumulateGrad object at 0x7ff25823cc10>, 0),)\n",
      "non_differentiable                      :None\n",
      "requires_grad                           :True\n",
      "saved_for_forward                       :None\n",
      "saved_tensors                           :(tensor(7.3891, grad_fn=<ExpBackward>),)\n",
      "saved_variables                         :(tensor(7.3891, grad_fn=<ExpBackward>),)\n",
      "to_save                                 :None\n",
      "After  out.backward() :  tensor(7.3891)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32725/2961181535.py:15: DeprecationWarning: 'saved_variables' is deprecated; use 'saved_tensors'\n",
      "  if not attr.startswith(\"__\") and not callable((a:=getattr(ctx,attr,None))):\n"
     ]
    }
   ],
   "source": [
    "print(\"Before out.backward() : \",in_tensor.grad)\n",
    "out_tensor.backward()\n",
    "print(\"After  out.backward() : \",in_tensor.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_tensor = torch.tensor(2.0, requires_grad=True)\n",
    "b_tensor = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "class Addition(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx,a,b):\n",
    "        return a+b\n",
    "    @staticmethod\n",
    "    def backward(ctx,grad):\n",
    "        # print(ctx.needs_input_grad)\n",
    "        # print(grad)\n",
    "        return grad,grad\n",
    "        \n",
    "\n",
    "c_tensor = Addition.apply(a_tensor,b_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4., grad_fn=<AdditionBackward>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None None\n",
      "tensor(1.) tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "print(a_tensor.grad,b_tensor.grad)\n",
    "c_tensor.backward(retain_graph=True)\n",
    "print(a_tensor.grad,b_tensor.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.) tensor(1.)\n",
      "tensor(6.) tensor(6.)\n"
     ]
    }
   ],
   "source": [
    "d_tensor = 5 * c_tensor\n",
    "\n",
    "print(a_tensor.grad,b_tensor.grad)\n",
    "d_tensor.backward(retain_graph=True)\n",
    "print(a_tensor.grad,b_tensor.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
